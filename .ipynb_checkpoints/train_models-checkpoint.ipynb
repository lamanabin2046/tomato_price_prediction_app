{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895e6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "except Exception:\n",
    "    XGBRegressor = None\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "except Exception:\n",
    "    LGBMRegressor = None\n",
    "    \n",
    "    \n",
    "try:\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "except Exception:\n",
    "    ARIMA = SARIMAX = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373928ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "np.random.seed(42)\n",
    "\n",
    "folder_tag=\"\" # modify path if lag features are used / planned to be used when lag features are not used\n",
    "\n",
    "\n",
    "model_plot_path=f\"artifacts/model_plots{folder_tag}/\"\n",
    "model_train_path=f\"artifacts/model_train_data{folder_tag}/\"\n",
    "model_path=f\"artifacts/models{folder_tag}/\"\n",
    "model_results_path=f\"artifacts/model_results{folder_tag}/\"\n",
    "\n",
    "# Create folders for artifacts & models\n",
    "os.makedirs(model_plot_path, exist_ok=True)\n",
    "os.makedirs(model_train_path, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "os.makedirs(model_results_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00723b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV = \"artifacts/data/clean_data.csv\"  # change as needed\n",
    "\n",
    "# Target variable name (ensure matches csv)\n",
    "TARGET = \"Average_Price\"\n",
    "\n",
    "# Which models to run (strings): \"Linear\", \"RandomForest\", \"XGBoost\", \"LightGBM\", \"ARIMA\"\n",
    "RUN_MODELS = [\"Linear\", \"RandomForest\", \"XGBoost\", \"LightGBM\", \"ARIMA\"]\n",
    "\n",
    "# TimeSeriesSplit folds\n",
    "N_SPLITS = 5\n",
    "\n",
    "# CV scoring metrics list (modify as needed)\n",
    "# Will be used for final reporting. Keep names consistent with regression_metrics below.\n",
    "METRIC_NAMES = [\"RMSE\", \"MSE\", \"MAE\", \"MAPE\", \"R2\"]\n",
    "\n",
    "# Randomized / Grid search settings\n",
    "RANDOM_SEARCH_ITER = 20\n",
    "GRID_SEARCH_SMALL = True  # if True, run smaller grids to save time\n",
    "\n",
    "# Use n_jobs=1 to avoid multiprocessing issues in constrained environments\n",
    "# N_JOBS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81eef8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics(y_true, y_pred):\n",
    "    \"\"\"Return dictionary of metrics for numeric arrays/series.\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    # safe MAPE\n",
    "    mask = y_true != 0\n",
    "    mape = np.nan\n",
    "    if mask.sum() > 0:\n",
    "        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"RMSE\": rmse, \"MSE\": mse, \"MAE\": mae, \"MAPE\": mape, \"R2\": r2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ce26be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn scorers (note: sklearn expects higher-is-better)\n",
    "sk_rmse = make_scorer(lambda y, yhat: -np.sqrt(mean_squared_error(y, yhat)))  # negative RMSE\n",
    "sk_mse  = make_scorer(lambda y, yhat: -mean_squared_error(y, yhat))\n",
    "sk_mae  = make_scorer(lambda y, yhat: -mean_absolute_error(y, yhat))\n",
    "def sk_mape(y, yhat):\n",
    "    mask = y != 0\n",
    "    if mask.sum()==0:\n",
    "        return 0.0\n",
    "    return -np.mean(np.abs((y[mask] - yhat[mask]) / y[mask])) * 100\n",
    "sk_mape_scorer = make_scorer(sk_mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ad560ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map names to scorers for GridSearchCV\n",
    "SCORING = {\n",
    "    \"neg_rmse\": sk_rmse,\n",
    "    \"neg_mse\": sk_mse,\n",
    "    \"neg_mae\": sk_mae,\n",
    "    \"neg_mape\": sk_mape_scorer,\n",
    "    \"r2\": \"r2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0380be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(fig, name):\n",
    "    filepath = os.path.join(model_plot_path, f\"{name}.png\")\n",
    "    fig.savefig(filepath, bbox_inches=\"tight\", dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(\"Saved:\", filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33db26d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: artifacts/data/clean_data.csv  shape: (1389, 23)\n"
     ]
    }
   ],
   "source": [
    "# --------- 3. Load data ---------\n",
    "df = pd.read_csv(INPUT_CSV, parse_dates=[\"Date\"], infer_datetime_format=True)\n",
    "print(\"Loaded:\", INPUT_CSV, \" shape:\", df.shape)\n",
    "\n",
    "# Quick drop rows with missing target\n",
    "df = df.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "bool_cols = df.select_dtypes(include=[\"bool\"]).columns\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "# Separate features & target, keep Date for potential time-splits/plots\n",
    "if \"Date\" in df.columns:\n",
    "    df = df.sort_values(\"Date\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26691064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_lag = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12284073",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_all = [c for c in df.columns if c not in [TARGET]]\n",
    "\n",
    "# # Exclude direct leakage: if any column equal to TARGET or 'imported_tomato_price' recorded same time, you can adjust here\n",
    "# if \"imported_tomato_price\" in cols_all:\n",
    "#     # drop from modeling if same-day leak for forecasting; user insisted earlier that it's leakage for forecasting\n",
    "#     cols_all.remove(\"imported_tomato_price\")\n",
    "#     print(\"Removed 'imported_tomato_price' from features (contemporaneous leak).\")\n",
    "\n",
    "# Drop Date from features\n",
    "if \"Date\" in cols_all:\n",
    "    cols_all.remove(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efaa03eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMERIC cols: ['Supply_Volume', 'Temperature', 'Precipitation', 'Wind_Speed', 'Air_Pressure', 'Rainfall_MM', 'USD_TO_NPR', 'Diesel', 'is_festival', 'imported_tomato_price', 'Inflation', 'day', 'month', 'day_of_week', 'is_weekend', 'month_sin', 'month_cos']\n",
      "CATEGORICAL cols: ['Season_Autumn', 'Season_Monsoon', 'Season_Spring', 'Season_Winter']\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric and categorical automatically\n",
    "numeric_cols = df[cols_all].select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in cols_all if c not in numeric_cols]\n",
    "\n",
    "print(\"NUMERIC cols:\", numeric_cols)\n",
    "print(\"CATEGORICAL cols:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94d1bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For no-lag experiment, optionally drop lag columns (heuristic: columns with '_lag' or '_roll' in name)\n",
    "# def filter_no_lag_columns(cols):\n",
    "#     return [c for c in cols if ((\"_lag\" not in c) and (\"_roll\" not in c) and (\"lag_\" not in c) and (\"roll\" not in c))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a4ee761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES_FULL count: 21\n"
     ]
    }
   ],
   "source": [
    "# Create feature sets\n",
    "FEATURES_FULL = cols_all                                    # all features present in CSV\n",
    "# FEATURES_NO_LAG = filter_no_lag_columns(FEATURES_FULL)      # remove lag/roll columns for no-lag model\n",
    "\n",
    "print(\"FEATURES_FULL count:\", len(FEATURES_FULL))\n",
    "# print(\"FEATURES_NO_LAG count:\", len(FEATURES_NO_LAG))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "476ac4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMERIC: 21 | CATEGORICAL: 0\n",
      "Preprocessor saved to artifacts/preprocessor/preprocessor.pkl\n"
     ]
    }
   ],
   "source": [
    "# Numeric pipeline: impute (ffill is already used earlier but we still include a safe imputer) + scaling\n",
    "def build_and_save_preprocessor(df, cols_all, save_path=\"artifacts/preprocessor/preprocessor.pkl\"):\n",
    "    # Auto-detect numeric & categorical\n",
    "    numeric_cols = df[cols_all].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = [c for c in cols_all if c not in numeric_cols]\n",
    "\n",
    "    print(f\"NUMERIC: {len(numeric_cols)} | CATEGORICAL: {len(cat_cols)}\")\n",
    "\n",
    "    # Numeric pipeline\n",
    "    num_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Categorical pipeline\n",
    "    cat_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    # ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        (\"num\", num_pipeline, numeric_cols),\n",
    "        (\"cat\", cat_pipeline, cat_cols)\n",
    "    ], remainder=\"drop\", sparse_threshold=0)\n",
    "\n",
    "    # Fit it once so it knows categories and feature order\n",
    "    preprocessor.fit(df[cols_all])\n",
    "\n",
    "    # Save both preprocessor and schema\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    joblib.dump(preprocessor, save_path)\n",
    "    joblib.dump({\n",
    "        \"numeric_cols\": numeric_cols,\n",
    "        \"cat_cols\": cat_cols,\n",
    "        \"all_cols\": cols_all\n",
    "    }, os.path.join(os.path.dirname(save_path), \"feature_schema.pkl\"))\n",
    "\n",
    "    print(f\"Preprocessor saved to {save_path}\")\n",
    "    return preprocessor\n",
    "\n",
    "# Helper to build a full sklearn Pipeline for a given estimator\n",
    "preprocessor=build_and_save_preprocessor(df,cols_all)\n",
    "\n",
    "def make_pipeline(estimator):\n",
    "    return Pipeline([\n",
    "        (\"preproc\", preprocessor),\n",
    "        (\"est\", estimator)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f4fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features count: 21\n",
      "X shape: (1389, 21)  y shape: (1389,)\n",
      "\n",
      "========================================\n",
      "Training model: Linear\n",
      "========================================\n",
      "No hyperparameter grid for Linear  — running cross_validate with TimeSeriesSplit.\n",
      "Saved model: artifacts/models/Linear_best.joblib\n",
      "\n",
      "========================================\n",
      "Training model: RandomForest\n",
      "========================================\n",
      "Running RandomizedSearchCV (broad) for RandomForest\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "param_grids = {}\n",
    "\n",
    "# 6.1 Linear\n",
    "models[\"Linear\"] = make_pipeline(LinearRegression())\n",
    "param_grids[\"Linear\"] = {\n",
    "    # Linear has no hyperparams here; include Ridge variant via separate model if needed\n",
    "}\n",
    "\n",
    "# 6.2 RandomForest\n",
    "models[\"RandomForest\"] = make_pipeline(RandomForestRegressor(random_state=42))\n",
    "param_grids[\"RandomForest\"] = {\n",
    "    \"est__n_estimators\": [100, 200] if GRID_SEARCH_SMALL else [100, 200, 400],\n",
    "    \"est__max_depth\": [5, 10, None],\n",
    "    \"est__min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "# 6.3 XGBoost (if available)\n",
    "if XGBRegressor is not None:\n",
    "    models[\"XGBoost\"] = make_pipeline(XGBRegressor(objective=\"reg:squarederror\", random_state=42))\n",
    "    param_grids[\"XGBoost\"] = {\n",
    "        \"est__n_estimators\": [100, 200],\n",
    "        \"est__max_depth\": [3, 5],\n",
    "        \"est__learning_rate\": [0.01, 0.05, 0.1]\n",
    "    }\n",
    "else:\n",
    "    print(\"XGBoost not available in this environment; skipping XGBoost.\")\n",
    "\n",
    "# 6.4 LightGBM (if available)\n",
    "if LGBMRegressor is not None:\n",
    "    models[\"LightGBM\"] = make_pipeline(LGBMRegressor(random_state=42))\n",
    "    param_grids[\"LightGBM\"] = {\n",
    "        \"est__n_estimators\": [100, 200],\n",
    "        \"est__num_leaves\": [31, 64],\n",
    "        \"est__learning_rate\": [0.01, 0.05]\n",
    "    }\n",
    "else:\n",
    "    print(\"LightGBM not available in this environment; skipping LightGBM.\")\n",
    "\n",
    "# 6.5 ARIMA/SARIMA handled separately (not inside sklearn pipeline)\n",
    "if ARIMA is None:\n",
    "    print(\"statsmodels ARIMA/SARIMAX not available; ARIMA steps will be skipped.\")\n",
    "\n",
    "# --------- 7. Train/validate with TimeSeriesSplit + hyperparameter tuning ---------\n",
    "# Choose which feature set to use: full (with lags) or no-lag\n",
    "USE_LAG_FEATURES = True  # change to False to force 'no-lag' experiment\n",
    "FEATURES_TO_USE = FEATURES_FULL \n",
    "print(\"Using features count:\", len(FEATURES_TO_USE))\n",
    "\n",
    "X = df[FEATURES_TO_USE].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "bool_cols = X.select_dtypes(include=[\"bool\"]).columns\n",
    "X[bool_cols] = X[bool_cols].astype(int)\n",
    "\n",
    "# Ensure no NA in X due to feature selection; imputer in pipeline handles remaining NAs.\n",
    "print(\"X shape:\", X.shape, \" y shape:\", y.shape)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "\n",
    "# Storage for results\n",
    "cv_summary = []\n",
    "best_models = {}\n",
    "\n",
    "# Iterate models for training and hyperparameter tuning\n",
    "for name, pipeline in models.items():\n",
    "    if name not in RUN_MODELS:\n",
    "        continue\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Training model:\", name)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    grid = param_grids.get(name, None)\n",
    "\n",
    "    # If no grid (e.g., Linear), just cross-validate without search\n",
    "    if not grid:\n",
    "        print(\"No hyperparameter grid for\", name, \" — running cross_validate with TimeSeriesSplit.\")\n",
    "        cv_res = cross_validate(\n",
    "        pipeline, X, y,\n",
    "        cv=tscv,\n",
    "        scoring=SCORING,\n",
    "        return_train_score=True\n",
    "        )\n",
    "\n",
    "        # Compute average metrics from cv_res (note neg scorers are negative)\n",
    "        # Convert negative scorers to positive metrics\n",
    "        results = {\n",
    "            \"Model\": name,\n",
    "            \"RMSE_mean\": -np.mean(cv_res[\"test_neg_rmse\"]) if \"test_neg_rmse\" in cv_res else np.nan,\n",
    "            \"MSE_mean\": -np.mean(cv_res[\"test_neg_mse\"]) if \"test_neg_mse\" in cv_res else np.nan,\n",
    "            \"MAE_mean\": -np.mean(cv_res[\"test_neg_mae\"]) if \"test_neg_mae\" in cv_res else np.nan,\n",
    "            \"MAPE_mean\": -np.mean(cv_res[\"test_neg_mape\"]) if \"test_neg_mape\" in cv_res else np.nan,\n",
    "            \"R2_mean\": np.mean(cv_res[\"test_r2\"]) if \"test_r2\" in cv_res else np.nan\n",
    "        }\n",
    "        cv_summary.append(results)\n",
    "\n",
    "        # Fit on full training portion (we will treat the last 20% as test later)\n",
    "        fitted = pipeline.fit(X, y)\n",
    "        best_models[name] = fitted\n",
    "        # Save fitted model\n",
    "        joblib.dump(fitted, f\"{model_path}{name}_best.joblib\")\n",
    "        print(\"Saved model:\", f\"{model_path}{name}_best.joblib\")\n",
    "        continue\n",
    "\n",
    "    # If grid provided -> run RandomizedSearch then refine with GridSearch (optional)\n",
    "    # Randomized Search (broad)\n",
    "    print(\"Running RandomizedSearchCV (broad) for\", name)\n",
    "    rnd = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=grid,\n",
    "        n_iter=RANDOM_SEARCH_ITER,\n",
    "        cv=tscv,\n",
    "        scoring=\"neg_mean_squared_error\",   # use neg MSE for search ranking\n",
    "        random_state=42,\n",
    "        # n_jobs=N_JOBS,\n",
    "        verbose=1\n",
    "    )\n",
    "    rnd.fit(X, y)\n",
    "    print(\"RandomSearch best params:\", rnd.best_params_, \" best_score:\", rnd.best_score_)\n",
    "\n",
    "    # Optionally run a smaller GridSearch around the best params (if desired)\n",
    "    # We will attempt a small grid: replace the param with the found best if not present\n",
    "    if GRID_SEARCH_SMALL:\n",
    "        # build a small grid based on rnd.best_params_ if possible\n",
    "        small_grid = {}\n",
    "        for k, v in grid.items():\n",
    "            if k in rnd.best_params_:\n",
    "                # if the best param is inside the list of grid values, pick neighbors or keep list small\n",
    "                small_choices = grid[k]\n",
    "                small_grid[k] = small_choices if len(small_choices) <= 3 else small_choices[:3]\n",
    "            else:\n",
    "                small_grid[k] = grid[k] if isinstance(grid[k], list) else [grid[k]]\n",
    "        print(\"Running GridSearchCV (refined) for\", name)\n",
    "        gscv = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=small_grid,\n",
    "            cv=tscv,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            # n_jobs=N_JOBS,\n",
    "            verbose=1\n",
    "        )\n",
    "        gscv.fit(X, y)\n",
    "        best_est = gscv.best_estimator_\n",
    "        best_params = gscv.best_params_\n",
    "        best_score = gscv.best_score_\n",
    "        print(\"GridSearch best params:\", best_params, \"best_score:\", best_score)\n",
    "    else:\n",
    "        best_est = rnd.best_estimator_\n",
    "        best_params = rnd.best_params_\n",
    "        best_score = rnd.best_score_\n",
    "\n",
    "    # Store best estimator\n",
    "    best_models[name] = best_est\n",
    "    # Save model to disk\n",
    "    joblib.dump(best_est, f\"{model_path}{name}_best.joblib\")\n",
    "    print(\"Saved best model:\", f\"{model_path}{name}_best.joblib\")\n",
    "\n",
    "    # Cross-validate the best estimator to get metrics per-fold\n",
    "    cv_res = cross_validate(best_est, X, y, cv=tscv, scoring=SCORING, return_train_score=False)\n",
    "    results = {\n",
    "        \"Model\": name,\n",
    "        \"RMSE_mean\": -np.mean(cv_res[\"test_neg_rmse\"]) if \"test_neg_rmse\" in cv_res else np.nan,\n",
    "        \"RMSE_std\" : np.std([-np.mean(cv_res[\"test_neg_rmse\"])]),\n",
    "        \"MSE_mean\": -np.mean(cv_res[\"test_neg_mse\"]) if \"test_neg_mse\" in cv_res else np.nan,\n",
    "        \"MAE_mean\": -np.mean(cv_res[\"test_neg_mae\"]) if \"test_neg_mae\" in cv_res else np.nan,\n",
    "        \"MAPE_mean\": -np.mean(cv_res[\"test_neg_mape\"]) if \"test_neg_mape\" in cv_res else np.nan,\n",
    "        \"R2_mean\": np.mean(cv_res[\"test_r2\"]) if \"test_r2\" in cv_res else np.nan\n",
    "    }\n",
    "    cv_summary.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07788187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating train vs CV metrics for Linear\n",
      "Evaluating train vs CV metrics for RandomForest\n",
      "Evaluating train vs CV metrics for XGBoost\n",
      "Evaluating train vs CV metrics for LightGBM\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 477\n",
      "[LightGBM] [Info] Number of data points in the train set: 234, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 74.712521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 759\n",
      "[LightGBM] [Info] Number of data points in the train set: 465, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 64.386215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1206\n",
      "[LightGBM] [Info] Number of data points in the train set: 696, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 65.971709\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1424\n",
      "[LightGBM] [Info] Number of data points in the train set: 927, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 64.453754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1632\n",
      "[LightGBM] [Info] Number of data points in the train set: 1158, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 66.175958\n",
      "    Model Metric  Fold      Train         CV\n",
      "0  Linear   RMSE     1 -10.172350 -61.012886\n",
      "1  Linear   RMSE     2  -9.348872 -42.056819\n",
      "2  Linear   RMSE     3 -16.762224 -22.450115\n",
      "3  Linear   RMSE     4 -15.975600 -22.274546\n",
      "4  Linear   RMSE     5 -16.189338 -14.200245\n"
     ]
    }
   ],
   "source": [
    "# --- Train vs CV comparison (per model, per metric) ---\n",
    "\n",
    "records = []\n",
    "\n",
    "# rerun cross_validate to capture both train & test per fold\n",
    "for name, pipeline in models.items():\n",
    "    if name not in best_models:\n",
    "        continue\n",
    "    print(f\"Evaluating train vs CV metrics for {name}\")\n",
    "    cv_res = cross_validate(\n",
    "        pipeline,\n",
    "        X, y,\n",
    "        cv=tscv,\n",
    "        scoring={\n",
    "            \"rmse\": sk_rmse,\n",
    "            \"mse\": sk_mse,\n",
    "            \"mae\": sk_mae,\n",
    "            \"mape\": sk_mape_scorer,\n",
    "            \"r2\": \"r2\"\n",
    "        },\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    for metric_key in [\"rmse\", \"mse\", \"mae\", \"mape\", \"r2\"]:\n",
    "        train_metric = cv_res.get(f\"train_{metric_key}\", None)\n",
    "        test_metric = cv_res.get(f\"test_{metric_key}\", None)\n",
    "        if train_metric is None or test_metric is None:\n",
    "            continue\n",
    "\n",
    "        # reverse neg sign for errors\n",
    "        if \"neg\" in metric_key:\n",
    "            train_metric = -train_metric\n",
    "            test_metric = -test_metric\n",
    "\n",
    "        for fold, (tr, ts) in enumerate(zip(train_metric, test_metric), 1):\n",
    "            records.append({\n",
    "                \"Model\": name,\n",
    "                \"Metric\": metric_key.upper(),\n",
    "                \"Fold\": fold,\n",
    "                \"Train\": tr,\n",
    "                \"CV\": ts\n",
    "            })\n",
    "\n",
    "df_comp = pd.DataFrame(records)\n",
    "print(df_comp.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac115c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: artifacts/model_plots/train_vs_cv_rmse_by_model.png\n",
      "Saved: artifacts/model_plots/train_vs_cv_mse_by_model.png\n",
      "Saved: artifacts/model_plots/train_vs_cv_mae_by_model.png\n",
      "Saved: artifacts/model_plots/train_vs_cv_mape_by_model.png\n",
      "Saved: artifacts/model_plots/train_vs_cv_r2_by_model.png\n"
     ]
    }
   ],
   "source": [
    "for metric in df_comp[\"Metric\"].unique():\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    dfp = df_comp[df_comp[\"Metric\"] == metric]\n",
    "    df_melt = dfp.melt(\n",
    "        id_vars=[\"Model\", \"Fold\", \"Metric\"],\n",
    "        value_vars=[\"Train\", \"CV\"],\n",
    "        var_name=\"Set\",\n",
    "        value_name=\"Score\"\n",
    "    )\n",
    "    sns.barplot(x=\"Model\", y=\"Score\", hue=\"Set\", data=df_melt, ax=ax)\n",
    "    ax.set_title(f\"Train vs CV — {metric}\")\n",
    "    ax.set_ylabel(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    save_plot(fig, f\"train_vs_cv_{metric.lower()}_by_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d330ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the last 20% of chronological data as test set (time-based holdout)\n",
    "holdout_frac = 0.2\n",
    "n_holdout = int(len(X) * holdout_frac)\n",
    "if n_holdout < 1:\n",
    "    raise RuntimeError(\"Dataset too small for holdout fraction.\")\n",
    "\n",
    "train_idx = slice(0, len(X) - n_holdout)\n",
    "test_idx = slice(len(X) - n_holdout, len(X))\n",
    "\n",
    "X_train_final = X.iloc[train_idx].reset_index(drop=True)\n",
    "y_train_final = y.iloc[train_idx].reset_index(drop=True)\n",
    "X_test_final  = X.iloc[test_idx].reset_index(drop=True)\n",
    "y_test_final  = y.iloc[test_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d055528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final holdout sizes -> Train: (1112, 21)  Test: (277, 21)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal holdout sizes -> Train:\", X_train_final.shape, \" Test:\", X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2417d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on holdout: Linear\n",
      "Saved: artifacts/model_plots/actual_vs_pred_Linear.png\n",
      "\n",
      "Evaluating on holdout: RandomForest\n",
      "Saved: artifacts/model_plots/actual_vs_pred_RandomForest.png\n",
      "\n",
      "Evaluating on holdout: XGBoost\n",
      "Saved: artifacts/model_plots/actual_vs_pred_XGBoost.png\n",
      "\n",
      "Evaluating on holdout: LightGBM\n",
      "Saved: artifacts/model_plots/actual_vs_pred_LightGBM.png\n",
      "\n",
      "Saved holdout performance to artifacts/model_results/holdout_performance.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "RMSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAPE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "R2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ae3a52ff-44a5-4173-9c5c-8cf8d11dc832",
       "rows": [
        [
         "1",
         "6.0921",
         "37.1143",
         "4.0076",
         "7.3777",
         "0.8653",
         "RandomForest"
        ],
        [
         "3",
         "8.8729",
         "78.7281",
         "6.9349",
         "17.4839",
         "0.7143",
         "LightGBM"
        ],
        [
         "0",
         "9.764",
         "95.336",
         "7.6122",
         "16.3853",
         "0.6541",
         "Linear"
        ],
        [
         "2",
         "10.7368",
         "115.2794",
         "8.4566",
         "21.3059",
         "0.5817",
         "XGBoost"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0921</td>\n",
       "      <td>37.1143</td>\n",
       "      <td>4.0076</td>\n",
       "      <td>7.3777</td>\n",
       "      <td>0.8653</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.8729</td>\n",
       "      <td>78.7281</td>\n",
       "      <td>6.9349</td>\n",
       "      <td>17.4839</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.7640</td>\n",
       "      <td>95.3360</td>\n",
       "      <td>7.6122</td>\n",
       "      <td>16.3853</td>\n",
       "      <td>0.6541</td>\n",
       "      <td>Linear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.7368</td>\n",
       "      <td>115.2794</td>\n",
       "      <td>8.4566</td>\n",
       "      <td>21.3059</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      RMSE       MSE     MAE     MAPE      R2         Model\n",
       "1   6.0921   37.1143  4.0076   7.3777  0.8653  RandomForest\n",
       "3   8.8729   78.7281  6.9349  17.4839  0.7143      LightGBM\n",
       "0   9.7640   95.3360  7.6122  16.3853  0.6541        Linear\n",
       "2  10.7368  115.2794  8.4566  21.3059  0.5817       XGBoost"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_records = []\n",
    "for name, model in best_models.items():\n",
    "    # Some entries may be raw pipelines already fitted or sklearn Pipelines unfit - ensure fitted\n",
    "    print(\"\\nEvaluating on holdout:\", name)\n",
    "    try:\n",
    "        # If model is a pipeline but not fitted, fit on training portion\n",
    "        if hasattr(model, \"fit\") and (not hasattr(model, \"predict\") or getattr(model, \"steps\", None) is None):\n",
    "            model.fit(X_train_final, y_train_final)\n",
    "\n",
    "        # Fit if not fitted (safe)\n",
    "        try:\n",
    "            # predict directly\n",
    "            y_pred = model.predict(X_test_final)\n",
    "        except Exception:\n",
    "            # fit on training then predict\n",
    "            model.fit(X_train_final, y_train_final)\n",
    "            y_pred = model.predict(X_test_final)\n",
    "    except Exception as e:\n",
    "        print(\"Model\", name, \"failed on holdout:\", e)\n",
    "        continue\n",
    "\n",
    "    mets = regression_metrics(y_test_final, y_pred)\n",
    "    mets[\"Model\"] = name\n",
    "    eval_records.append(mets)\n",
    "\n",
    "    # Plot actual vs predicted on holdout\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    # If Date exists in df, display the last dates for x-axis\n",
    "    if \"Date\" in df.columns:\n",
    "        test_dates = df[\"Date\"].iloc[test_idx].reset_index(drop=True)\n",
    "        ax.plot(test_dates, y_test_final, label=\"Actual\")\n",
    "        ax.plot(test_dates, y_pred, linestyle=\"--\", label=\"Predicted\")\n",
    "        ax.set_xticklabels(test_dates.dt.strftime(\"%Y-%m-%d\"), rotation=45)\n",
    "    else:\n",
    "        ax.plot(y_test_final.index, y_test_final, label=\"Actual\")\n",
    "        ax.plot(y_test_final.index, y_pred, linestyle=\"--\", label=\"Predicted\")\n",
    "    ax.set_title(f\"{name} — Actual vs Predicted (Holdout)\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    save_plot(fig, f\"actual_vs_pred_{name}\")\n",
    "\n",
    "# Save eval records to CSV\n",
    "eval_df = pd.DataFrame(eval_records).sort_values(\"RMSE\")\n",
    "eval_df.to_csv(f\"{model_results_path}holdout_performance.csv\", index=False)\n",
    "print(f\"\\nSaved holdout performance to {model_results_path}holdout_performance.csv\")\n",
    "display(eval_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e0611c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CV summary to artifacts/model_results/cv_summary.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "RMSE_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MSE_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAE_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAPE_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "R2_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RMSE_std",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "19c32616-1f6f-4e4b-9a73-869166098c0d",
       "rows": [
        [
         "0",
         "Linear",
         "32.3989",
         "1338.6316",
         "25.5204",
         "45.0703",
         "-7.0747",
         null
        ],
        [
         "1",
         "RandomForest",
         "19.3191",
         "426.1227",
         "13.5582",
         "22.9737",
         "-0.2748",
         "0.0"
        ],
        [
         "2",
         "XGBoost",
         "19.3684",
         "406.7467",
         "15.2552",
         "28.0424",
         "-0.2991",
         "0.0"
        ],
        [
         "3",
         "LightGBM",
         "18.8791",
         "399.0302",
         "14.3365",
         "25.7092",
         "-0.2121",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE_mean</th>\n",
       "      <th>MSE_mean</th>\n",
       "      <th>MAE_mean</th>\n",
       "      <th>MAPE_mean</th>\n",
       "      <th>R2_mean</th>\n",
       "      <th>RMSE_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear</td>\n",
       "      <td>32.3989</td>\n",
       "      <td>1338.6316</td>\n",
       "      <td>25.5204</td>\n",
       "      <td>45.0703</td>\n",
       "      <td>-7.0747</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>19.3191</td>\n",
       "      <td>426.1227</td>\n",
       "      <td>13.5582</td>\n",
       "      <td>22.9737</td>\n",
       "      <td>-0.2748</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>19.3684</td>\n",
       "      <td>406.7467</td>\n",
       "      <td>15.2552</td>\n",
       "      <td>28.0424</td>\n",
       "      <td>-0.2991</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>18.8791</td>\n",
       "      <td>399.0302</td>\n",
       "      <td>14.3365</td>\n",
       "      <td>25.7092</td>\n",
       "      <td>-0.2121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  RMSE_mean   MSE_mean  MAE_mean  MAPE_mean  R2_mean  RMSE_std\n",
       "0        Linear    32.3989  1338.6316   25.5204    45.0703  -7.0747       NaN\n",
       "1  RandomForest    19.3191   426.1227   13.5582    22.9737  -0.2748       0.0\n",
       "2       XGBoost    19.3684   406.7467   15.2552    28.0424  -0.2991       0.0\n",
       "3      LightGBM    18.8791   399.0302   14.3365    25.7092  -0.2121       0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: artifacts/model_plots/cv_rmse_by_model.png\n",
      "Saved: artifacts/model_plots/holdout_rmse_by_model.png\n"
     ]
    }
   ],
   "source": [
    "cv_df = pd.DataFrame(cv_summary)\n",
    "cv_df.to_csv(f\"{model_results_path}cv_summary.csv\", index=False)\n",
    "print(f\"Saved CV summary to {model_results_path}cv_summary.csv\")\n",
    "display(cv_df.round(4))\n",
    "\n",
    "# Plot CV RMSE comparison\n",
    "if \"RMSE_mean\" in cv_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    sns.barplot(x=\"Model\", y=\"RMSE_mean\", data=cv_df, dodge=False, ax=ax)\n",
    "    ax.set_title(\"CV: Mean RMSE by Model\")\n",
    "    save_plot(fig, \"cv_rmse_by_model\")\n",
    "\n",
    "# Plot holdout RMSE\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "sns.barplot(x=\"Model\", y=\"RMSE\", data=eval_df.rename(columns={\"RMSE\":\"RMSE\"}), dodge=False, ax=ax)\n",
    "ax.set_title(\"Holdout: RMSE by Model\")\n",
    "save_plot(fig, \"holdout_rmse_by_model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32d82fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: artifacts/model_plots/rf_top_feature_importances.png\n"
     ]
    }
   ],
   "source": [
    "# --------- 10. Feature importance for tree models (Permutation or built-in) ---------\n",
    "# If RandomForest present, extract feature importances (via fitted pipeline)\n",
    "if \"RandomForest\" in best_models:\n",
    "    rf = best_models[\"RandomForest\"]\n",
    "    try:\n",
    "        # Get feature names after preprocessing\n",
    "        pre = rf.named_steps[\"preproc\"]\n",
    "        # numeric names and onehot names\n",
    "        num_names = numeric_cols\n",
    "        # get onehot feature names if any categories\n",
    "        try:\n",
    "            ohe = pre.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "            ohe_names = list(ohe.get_feature_names_out(cat_cols))\n",
    "        except Exception:\n",
    "            ohe_names = []\n",
    "        feat_names = num_names + ohe_names\n",
    "\n",
    "        # Extract underlying RandomForest\n",
    "        rf_est = rf.named_steps[\"est\"]\n",
    "        importances = getattr(rf_est, \"feature_importances_\", None)\n",
    "        if importances is not None:\n",
    "            imp_df = pd.DataFrame({\"feature\": feat_names, \"importance\": importances})\n",
    "            imp_df = imp_df.sort_values(\"importance\", ascending=False).head(30)\n",
    "            imp_df.to_csv(\"artifacts/model_results/rf_feature_importances.csv\", index=False)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(8,6))\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=imp_df, ax=ax)\n",
    "            ax.set_title(\"RandomForest: Top feature importances\")\n",
    "            save_plot(fig, \"rf_top_feature_importances\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to extract RF feature importances:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ae8fe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running simple ARIMA baseline on target (univariate) ...\n",
      "Saved: artifacts/model_plots/arima_holdout.png\n",
      "Saved ARIMA baseline model.\n"
     ]
    }
   ],
   "source": [
    "if \"ARIMA\" in RUN_MODELS and ARIMA is not None:\n",
    "    print(\"\\nRunning simple ARIMA baseline on target (univariate) ...\")\n",
    "    try:\n",
    "        series = df.set_index(\"Date\")[TARGET].asfreq(\"D\").interpolate()\n",
    "        arima_order = (1,1,1)   # simple baseline\n",
    "        arima_model = ARIMA(series.iloc[:-n_holdout], order=arima_order).fit()\n",
    "        arima_fore = arima_model.forecast(steps=n_holdout)\n",
    "        mets = regression_metrics(series.iloc[-n_holdout:].values, arima_fore)\n",
    "        mets[\"Model\"] = \"ARIMA\"\n",
    "        # append to eval records and save plot\n",
    "        eval_df = pd.concat([eval_df, pd.DataFrame([mets])], ignore_index=True)\n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(10,3))\n",
    "        ax.plot(series.index[-n_holdout:], series.iloc[-n_holdout:], label=\"Actual\")\n",
    "        ax.plot(series.index[-n_holdout:], arima_fore, linestyle=\"--\", label=\"ARIMA_Forecast\")\n",
    "        ax.set_title(\"ARIMA: Actual vs Forecast (holdout)\")\n",
    "        ax.legend()\n",
    "        save_plot(fig, \"arima_holdout\")\n",
    "        # Save ARIMA model\n",
    "        joblib.dump(arima_model, \"artifacts/models/arima_baseline.joblib\")\n",
    "        print(\"Saved ARIMA baseline model.\")\n",
    "    except Exception as e:\n",
    "        print(\"ARIMA failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10f0a848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved manifest -> artifacts/model_results/manifest.json\n",
      "\n",
      "Notebook 2 complete. Artifacts saved under artifacts/model_plots, artifacts/models, artifacts/model_results.\n"
     ]
    }
   ],
   "source": [
    "manifest = {\n",
    "    \"timestamp\": datetime.utcnow().isoformat(),\n",
    "    \"input_csv\": INPUT_CSV,\n",
    "    \"target\": TARGET,\n",
    "    \"features_used\": FEATURES_TO_USE,\n",
    "    \"numeric_cols\": numeric_cols,\n",
    "    \"cat_cols\": cat_cols,\n",
    "    \"models_trained\": list(best_models.keys()),\n",
    "    \"holdout_rows\": int(n_holdout)\n",
    "}\n",
    "with open(f\"{model_results_path}manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(f\"Saved manifest -> {model_results_path}manifest.json\")\n",
    "\n",
    "print(\"\\nNotebook 2 complete. Artifacts saved under artifacts/model_plots, artifacts/models, artifacts/model_results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77bdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
