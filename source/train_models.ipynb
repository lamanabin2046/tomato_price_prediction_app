{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895e6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "except Exception:\n",
    "    XGBRegressor = None\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "except Exception:\n",
    "    LGBMRegressor = None\n",
    "    \n",
    "    \n",
    "try:\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "except Exception:\n",
    "    ARIMA = SARIMAX = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373928ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "np.random.seed(42)\n",
    "\n",
    "folder_tag=\"_lag\" # modify path if lag features are used / planned to be used when lag features are not used\n",
    "\n",
    "\n",
    "model_plot_path=f\"artifacts/model_plots{folder_tag}/\"\n",
    "model_path=f\"artifacts/models{folder_tag}/\"\n",
    "model_results_path=f\"artifacts/model_results{folder_tag}/\"\n",
    "\n",
    "# Create folders for artifacts & models\n",
    "os.makedirs(model_plot_path, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "os.makedirs(model_results_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00723b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV = \"artifacts/data/clean_data_with_lag_roll.csv\"  # change as needed\n",
    "\n",
    "# Target variable name (ensure matches csv)\n",
    "TARGET = \"Average_Price\"\n",
    "\n",
    "# Which models to run (strings): \"Linear\", \"RandomForest\", \"XGBoost\", \"LightGBM\", \"ARIMA\"\n",
    "RUN_MODELS = [\"Linear\", \"RandomForest\", \"XGBoost\", \"LightGBM\", \"ARIMA\"]\n",
    "\n",
    "# TimeSeriesSplit folds\n",
    "N_SPLITS = 5\n",
    "\n",
    "# CV scoring metrics list (modify as needed)\n",
    "# Will be used for final reporting. Keep names consistent with regression_metrics below.\n",
    "METRIC_NAMES = [\"RMSE\", \"MSE\", \"MAE\", \"MAPE\", \"R2\"]\n",
    "\n",
    "# Randomized / Grid search settings\n",
    "RANDOM_SEARCH_ITER = 20\n",
    "GRID_SEARCH_SMALL = True  # if True, run smaller grids to save time\n",
    "\n",
    "# Use n_jobs=1 to avoid multiprocessing issues in constrained environments\n",
    "# N_JOBS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81eef8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics(y_true, y_pred):\n",
    "    \"\"\"Return dictionary of metrics for numeric arrays/series.\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    # safe MAPE\n",
    "    mask = y_true != 0\n",
    "    mape = np.nan\n",
    "    if mask.sum() > 0:\n",
    "        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"RMSE\": rmse, \"MSE\": mse, \"MAE\": mae, \"MAPE\": mape, \"R2\": r2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ce26be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn scorers (note: sklearn expects higher-is-better)\n",
    "sk_rmse = make_scorer(lambda y, yhat: -np.sqrt(mean_squared_error(y, yhat)))  # negative RMSE\n",
    "sk_mse  = make_scorer(lambda y, yhat: -mean_squared_error(y, yhat))\n",
    "sk_mae  = make_scorer(lambda y, yhat: -mean_absolute_error(y, yhat))\n",
    "def sk_mape(y, yhat):\n",
    "    mask = y != 0\n",
    "    if mask.sum()==0:\n",
    "        return 0.0\n",
    "    return -np.mean(np.abs((y[mask] - yhat[mask]) / y[mask])) * 100\n",
    "sk_mape_scorer = make_scorer(sk_mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad560ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map names to scorers for GridSearchCV\n",
    "SCORING = {\n",
    "    \"neg_rmse\": sk_rmse,\n",
    "    \"neg_mse\": sk_mse,\n",
    "    \"neg_mae\": sk_mae,\n",
    "    \"neg_mape\": sk_mape_scorer,\n",
    "    \"r2\": \"r2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0380be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(fig, name):\n",
    "    filepath = os.path.join(model_plot_path, f\"{name}.png\")\n",
    "    fig.savefig(filepath, bbox_inches=\"tight\", dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(\"Saved:\", filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33db26d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: artifacts/data/clean_data_with_lag_roll.csv  shape: (1300, 35)\n"
     ]
    }
   ],
   "source": [
    "# --------- 3. Load data ---------\n",
    "df = pd.read_csv(INPUT_CSV, parse_dates=[\"Date\"], infer_datetime_format=True)\n",
    "print(\"Loaded:\", INPUT_CSV, \" shape:\", df.shape)\n",
    "\n",
    "# Quick drop rows with missing target\n",
    "df = df.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "# Separate features & target, keep Date for potential time-splits/plots\n",
    "if \"Date\" in df.columns:\n",
    "    df = df.sort_values(\"Date\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26691064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_lag = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12284073",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_all = [c for c in df.columns if c not in [TARGET]]\n",
    "\n",
    "# # Exclude direct leakage: if any column equal to TARGET or 'imported_tomato_price' recorded same time, you can adjust here\n",
    "# if \"imported_tomato_price\" in cols_all:\n",
    "#     # drop from modeling if same-day leak for forecasting; user insisted earlier that it's leakage for forecasting\n",
    "#     cols_all.remove(\"imported_tomato_price\")\n",
    "#     print(\"Removed 'imported_tomato_price' from features (contemporaneous leak).\")\n",
    "\n",
    "# Drop Date from features\n",
    "if \"Date\" in cols_all:\n",
    "    cols_all.remove(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efaa03eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMERIC cols: ['Supply_Volume', 'Temperature', 'Precipitation', 'Wind_Speed', 'Air_Pressure', 'Rainfall_MM', 'USD_TO_NPR', 'Diesel', 'is_festival', 'imported_tomato_price', 'Inflation', 'day', 'month', 'day_of_week', 'is_weekend', 'month_sin', 'month_cos', 'Average_Price_lag1', 'Average_Price_lag7', 'Average_Price_lag30', 'Average_Price_rollmean_7', 'Average_Price_rollmean_30', 'Average_Price_rollmean_90', 'Supply_Volume_rollmean_7', 'Supply_Volume_rollmean_30', 'Supply_Volume_rollmean_90', 'imported_tomato_price_rollmean_7', 'imported_tomato_price_rollmean_30', 'imported_tomato_price_rollmean_90']\n",
      "CATEGORICAL cols: ['Season_Autumn', 'Season_Monsoon', 'Season_Spring', 'Season_Winter']\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric and categorical automatically\n",
    "numeric_cols = df[cols_all].select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in cols_all if c not in numeric_cols]\n",
    "\n",
    "print(\"NUMERIC cols:\", numeric_cols)\n",
    "print(\"CATEGORICAL cols:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94d1bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For no-lag experiment, optionally drop lag columns (heuristic: columns with '_lag' or '_roll' in name)\n",
    "def filter_no_lag_columns(cols):\n",
    "    return [c for c in cols if ((\"_lag\" not in c) and (\"_roll\" not in c) and (\"lag_\" not in c) and (\"roll\" not in c))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a4ee761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES_FULL count: 33\n",
      "FEATURES_NO_LAG count: 21\n"
     ]
    }
   ],
   "source": [
    "# Create feature sets\n",
    "FEATURES_FULL = cols_all                                    # all features present in CSV\n",
    "FEATURES_NO_LAG = filter_no_lag_columns(FEATURES_FULL)      # remove lag/roll columns for no-lag model\n",
    "\n",
    "print(\"FEATURES_FULL count:\", len(FEATURES_FULL))\n",
    "print(\"FEATURES_NO_LAG count:\", len(FEATURES_NO_LAG))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476ac4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric pipeline: impute (ffill is already used earlier but we still include a safe imputer) + scaling\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),   # safe; data should already be filled but keep for robustness\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: impute -> one-hot (if any categories)\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", num_pipeline, numeric_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "], remainder=\"drop\", sparse_threshold=0)\n",
    "\n",
    "# Helper to build a full sklearn Pipeline for a given estimator\n",
    "def make_pipeline(estimator):\n",
    "    return Pipeline([\n",
    "        (\"preproc\", preprocessor),\n",
    "        (\"est\", estimator)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f4fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features count: 33\n",
      "X shape: (1300, 33)  y shape: (1300,)\n",
      "\n",
      "========================================\n",
      "Training model: Linear\n",
      "========================================\n",
      "No hyperparameter grid for Linear  — running cross_validate with TimeSeriesSplit.\n",
      "Saved model: artifacts/models_lag/Linear_fitted.joblib\n",
      "\n",
      "========================================\n",
      "Training model: RandomForest\n",
      "========================================\n",
      "Running RandomizedSearchCV (broad) for RandomForest\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "RandomSearch best params: {'est__n_estimators': 100, 'est__min_samples_leaf': 4, 'est__max_depth': 5}  best_score: -60.69094302122615\n",
      "Running GridSearchCV (refined) for RandomForest\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "GridSearch best params: {'est__max_depth': 5, 'est__min_samples_leaf': 4, 'est__n_estimators': 100} best_score: -60.69094302122615\n",
      "Saved best model: artifacts/models_lag/RandomForest_best.joblib\n",
      "\n",
      "========================================\n",
      "Training model: XGBoost\n",
      "========================================\n",
      "Running RandomizedSearchCV (broad) for XGBoost\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "RandomSearch best params: {'est__n_estimators': 100, 'est__max_depth': 3, 'est__learning_rate': 0.1}  best_score: -74.77217682489301\n",
      "Running GridSearchCV (refined) for XGBoost\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "param_grids = {}\n",
    "\n",
    "# 6.1 Linear\n",
    "models[\"Linear\"] = make_pipeline(LinearRegression())\n",
    "param_grids[\"Linear\"] = {\n",
    "    # Linear has no hyperparams here; include Ridge variant via separate model if needed\n",
    "}\n",
    "\n",
    "# 6.2 RandomForest\n",
    "models[\"RandomForest\"] = make_pipeline(RandomForestRegressor(random_state=42))\n",
    "param_grids[\"RandomForest\"] = {\n",
    "    \"est__n_estimators\": [100, 200] if GRID_SEARCH_SMALL else [100, 200, 400],\n",
    "    \"est__max_depth\": [5, 10, None],\n",
    "    \"est__min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "# 6.3 XGBoost (if available)\n",
    "if XGBRegressor is not None:\n",
    "    models[\"XGBoost\"] = make_pipeline(XGBRegressor(objective=\"reg:squarederror\", random_state=42))\n",
    "    param_grids[\"XGBoost\"] = {\n",
    "        \"est__n_estimators\": [100, 200],\n",
    "        \"est__max_depth\": [3, 5],\n",
    "        \"est__learning_rate\": [0.01, 0.05, 0.1]\n",
    "    }\n",
    "else:\n",
    "    print(\"XGBoost not available in this environment; skipping XGBoost.\")\n",
    "\n",
    "# 6.4 LightGBM (if available)\n",
    "if LGBMRegressor is not None:\n",
    "    models[\"LightGBM\"] = make_pipeline(LGBMRegressor(random_state=42))\n",
    "    param_grids[\"LightGBM\"] = {\n",
    "        \"est__n_estimators\": [100, 200],\n",
    "        \"est__num_leaves\": [31, 64],\n",
    "        \"est__learning_rate\": [0.01, 0.05]\n",
    "    }\n",
    "else:\n",
    "    print(\"LightGBM not available in this environment; skipping LightGBM.\")\n",
    "\n",
    "# 6.5 ARIMA/SARIMA handled separately (not inside sklearn pipeline)\n",
    "if ARIMA is None:\n",
    "    print(\"statsmodels ARIMA/SARIMAX not available; ARIMA steps will be skipped.\")\n",
    "\n",
    "# --------- 7. Train/validate with TimeSeriesSplit + hyperparameter tuning ---------\n",
    "# Choose which feature set to use: full (with lags) or no-lag\n",
    "USE_LAG_FEATURES = True  # change to False to force 'no-lag' experiment\n",
    "FEATURES_TO_USE = FEATURES_FULL if USE_LAG_FEATURES else FEATURES_NO_LAG\n",
    "print(\"Using features count:\", len(FEATURES_TO_USE))\n",
    "\n",
    "X = df[FEATURES_TO_USE].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "bool_cols = X.select_dtypes(include=[\"bool\"]).columns\n",
    "X[bool_cols] = X[bool_cols].astype(int)\n",
    "\n",
    "# Ensure no NA in X due to feature selection; imputer in pipeline handles remaining NAs.\n",
    "print(\"X shape:\", X.shape, \" y shape:\", y.shape)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "\n",
    "# Storage for results\n",
    "cv_summary = []\n",
    "best_models = {}\n",
    "\n",
    "# Iterate models for training and hyperparameter tuning\n",
    "for name, pipeline in models.items():\n",
    "    if name not in RUN_MODELS:\n",
    "        continue\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Training model:\", name)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    grid = param_grids.get(name, None)\n",
    "\n",
    "    # If no grid (e.g., Linear), just cross-validate without search\n",
    "    if not grid:\n",
    "        print(\"No hyperparameter grid for\", name, \" — running cross_validate with TimeSeriesSplit.\")\n",
    "        cv_res = cross_validate(\n",
    "        pipeline, X, y,\n",
    "        cv=tscv,\n",
    "        scoring=SCORING,\n",
    "        return_train_score=True\n",
    "        )\n",
    "\n",
    "        # Compute average metrics from cv_res (note neg scorers are negative)\n",
    "        # Convert negative scorers to positive metrics\n",
    "        results = {\n",
    "            \"Model\": name,\n",
    "            \"RMSE_mean\": -np.mean(cv_res[\"test_neg_rmse\"]) if \"test_neg_rmse\" in cv_res else np.nan,\n",
    "            \"MSE_mean\": -np.mean(cv_res[\"test_neg_mse\"]) if \"test_neg_mse\" in cv_res else np.nan,\n",
    "            \"MAE_mean\": -np.mean(cv_res[\"test_neg_mae\"]) if \"test_neg_mae\" in cv_res else np.nan,\n",
    "            \"MAPE_mean\": -np.mean(cv_res[\"test_neg_mape\"]) if \"test_neg_mape\" in cv_res else np.nan,\n",
    "            \"R2_mean\": np.mean(cv_res[\"test_r2\"]) if \"test_r2\" in cv_res else np.nan\n",
    "        }\n",
    "        cv_summary.append(results)\n",
    "\n",
    "        # Fit on full training portion (we will treat the last 20% as test later)\n",
    "        fitted = pipeline.fit(X, y)\n",
    "        best_models[name] = fitted\n",
    "        # Save fitted model\n",
    "        joblib.dump(fitted, f\"{model_path}{name}_fitted.joblib\")\n",
    "        print(\"Saved model:\", f\"{model_path}{name}_fitted.joblib\")\n",
    "        continue\n",
    "\n",
    "    # If grid provided -> run RandomizedSearch then refine with GridSearch (optional)\n",
    "    # Randomized Search (broad)\n",
    "    print(\"Running RandomizedSearchCV (broad) for\", name)\n",
    "    rnd = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=grid,\n",
    "        n_iter=RANDOM_SEARCH_ITER,\n",
    "        cv=tscv,\n",
    "        scoring=\"neg_mean_squared_error\",   # use neg MSE for search ranking\n",
    "        random_state=42,\n",
    "        # n_jobs=N_JOBS,\n",
    "        verbose=1\n",
    "    )\n",
    "    rnd.fit(X, y)\n",
    "    print(\"RandomSearch best params:\", rnd.best_params_, \" best_score:\", rnd.best_score_)\n",
    "\n",
    "    # Optionally run a smaller GridSearch around the best params (if desired)\n",
    "    # We will attempt a small grid: replace the param with the found best if not present\n",
    "    if GRID_SEARCH_SMALL:\n",
    "        # build a small grid based on rnd.best_params_ if possible\n",
    "        small_grid = {}\n",
    "        for k, v in grid.items():\n",
    "            if k in rnd.best_params_:\n",
    "                # if the best param is inside the list of grid values, pick neighbors or keep list small\n",
    "                small_choices = grid[k]\n",
    "                small_grid[k] = small_choices if len(small_choices) <= 3 else small_choices[:3]\n",
    "            else:\n",
    "                small_grid[k] = grid[k] if isinstance(grid[k], list) else [grid[k]]\n",
    "        print(\"Running GridSearchCV (refined) for\", name)\n",
    "        gscv = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=small_grid,\n",
    "            cv=tscv,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            # n_jobs=N_JOBS,\n",
    "            verbose=1\n",
    "        )\n",
    "        gscv.fit(X, y)\n",
    "        best_est = gscv.best_estimator_\n",
    "        best_params = gscv.best_params_\n",
    "        best_score = gscv.best_score_\n",
    "        print(\"GridSearch best params:\", best_params, \"best_score:\", best_score)\n",
    "    else:\n",
    "        best_est = rnd.best_estimator_\n",
    "        best_params = rnd.best_params_\n",
    "        best_score = rnd.best_score_\n",
    "\n",
    "    # Store best estimator\n",
    "    best_models[name] = best_est\n",
    "    # Save model to disk\n",
    "    joblib.dump(best_est, f\"{model_path}{name}_best.joblib\")\n",
    "    print(\"Saved best model:\", f\"{model_path}{name}_best.joblib\")\n",
    "\n",
    "    # Cross-validate the best estimator to get metrics per-fold\n",
    "    cv_res = cross_validate(best_est, X, y, cv=tscv, scoring=SCORING, return_train_score=False)\n",
    "    results = {\n",
    "        \"Model\": name,\n",
    "        \"RMSE_mean\": -np.mean(cv_res[\"test_neg_rmse\"]) if \"test_neg_rmse\" in cv_res else np.nan,\n",
    "        \"RMSE_std\" : np.std([-np.mean(cv_res[\"test_neg_rmse\"])]),\n",
    "        \"MSE_mean\": -np.mean(cv_res[\"test_neg_mse\"]) if \"test_neg_mse\" in cv_res else np.nan,\n",
    "        \"MAE_mean\": -np.mean(cv_res[\"test_neg_mae\"]) if \"test_neg_mae\" in cv_res else np.nan,\n",
    "        \"MAPE_mean\": -np.mean(cv_res[\"test_neg_mape\"]) if \"test_neg_mape\" in cv_res else np.nan,\n",
    "        \"R2_mean\": np.mean(cv_res[\"test_r2\"]) if \"test_r2\" in cv_res else np.nan\n",
    "    }\n",
    "    cv_summary.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07788187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating train vs CV metrics for Linear\n",
      "Evaluating train vs CV metrics for RandomForest\n",
      "Evaluating train vs CV metrics for XGBoost\n",
      "Evaluating train vs CV metrics for LightGBM\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1229\n",
      "[LightGBM] [Info] Number of data points in the train set: 220, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score 72.249091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2178\n",
      "[LightGBM] [Info] Number of data points in the train set: 436, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 61.383922\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3217\n",
      "[LightGBM] [Info] Number of data points in the train set: 652, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 65.244340\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3867\n",
      "[LightGBM] [Info] Number of data points in the train set: 868, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 64.607511\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4274\n",
      "[LightGBM] [Info] Number of data points in the train set: 1084, number of used features: 37\n",
      "[LightGBM] [Info] Start training from score 65.388109\n",
      "    Model Metric  Fold     Train         CV\n",
      "0  Linear   RMSE     1 -5.078945 -24.978563\n",
      "1  Linear   RMSE     2 -4.784838 -15.255625\n",
      "2  Linear   RMSE     3 -5.803603  -6.422183\n",
      "3  Linear   RMSE     4 -5.854873  -3.345814\n",
      "4  Linear   RMSE     5 -5.401603  -4.146999\n"
     ]
    }
   ],
   "source": [
    "# --- Train vs CV comparison (per model, per metric) ---\n",
    "\n",
    "records = []\n",
    "\n",
    "# rerun cross_validate to capture both train & test per fold\n",
    "for name, pipeline in models.items():\n",
    "    if name not in best_models:\n",
    "        continue\n",
    "    print(f\"Evaluating train vs CV metrics for {name}\")\n",
    "    cv_res = cross_validate(\n",
    "        pipeline,\n",
    "        X, y,\n",
    "        cv=tscv,\n",
    "        scoring={\n",
    "            \"rmse\": sk_rmse,\n",
    "            \"mse\": sk_mse,\n",
    "            \"mae\": sk_mae,\n",
    "            \"mape\": sk_mape_scorer,\n",
    "            \"r2\": \"r2\"\n",
    "        },\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    for metric_key in [\"rmse\", \"mse\", \"mae\", \"mape\", \"r2\"]:\n",
    "        train_metric = cv_res.get(f\"train_{metric_key}\", None)\n",
    "        test_metric = cv_res.get(f\"test_{metric_key}\", None)\n",
    "        if train_metric is None or test_metric is None:\n",
    "            continue\n",
    "\n",
    "        # reverse neg sign for errors\n",
    "        if \"neg\" in metric_key:\n",
    "            train_metric = -train_metric\n",
    "            test_metric = -test_metric\n",
    "\n",
    "        for fold, (tr, ts) in enumerate(zip(train_metric, test_metric), 1):\n",
    "            records.append({\n",
    "                \"Model\": name,\n",
    "                \"Metric\": metric_key.upper(),\n",
    "                \"Fold\": fold,\n",
    "                \"Train\": tr,\n",
    "                \"CV\": ts\n",
    "            })\n",
    "\n",
    "df_comp = pd.DataFrame(records)\n",
    "print(df_comp.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac115c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: artifacts/model_plots_lag/\n",
      "Saved: artifacts/model_plots_lag/\n",
      "Saved: artifacts/model_plots_lag/\n",
      "Saved: artifacts/model_plots_lag/\n",
      "Saved: artifacts/model_plots_lag/\n"
     ]
    }
   ],
   "source": [
    "for metric in df_comp[\"Metric\"].unique():\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    dfp = df_comp[df_comp[\"Metric\"] == metric]\n",
    "    df_melt = dfp.melt(\n",
    "        id_vars=[\"Model\", \"Fold\", \"Metric\"],\n",
    "        value_vars=[\"Train\", \"CV\"],\n",
    "        var_name=\"Set\",\n",
    "        value_name=\"Score\"\n",
    "    )\n",
    "    sns.barplot(x=\"Model\", y=\"Score\", hue=\"Set\", data=df_melt, ax=ax)\n",
    "    ax.set_title(f\"Train vs CV — {metric}\")\n",
    "    ax.set_ylabel(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    save_plot(fig, f\"train_vs_cv_{metric.lower()}_by_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d330ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the last 20% of chronological data as test set (time-based holdout)\n",
    "holdout_frac = 0.2\n",
    "n_holdout = int(len(X) * holdout_frac)\n",
    "if n_holdout < 1:\n",
    "    raise RuntimeError(\"Dataset too small for holdout fraction.\")\n",
    "\n",
    "train_idx = slice(0, len(X) - n_holdout)\n",
    "test_idx = slice(len(X) - n_holdout, len(X))\n",
    "\n",
    "X_train_final = X.iloc[train_idx].reset_index(drop=True)\n",
    "y_train_final = y.iloc[train_idx].reset_index(drop=True)\n",
    "X_test_final  = X.iloc[test_idx].reset_index(drop=True)\n",
    "y_test_final  = y.iloc[test_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d055528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final holdout sizes -> Train: (1040, 33)  Test: (260, 33)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal holdout sizes -> Train:\", X_train_final.shape, \" Test:\", X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on holdout: Linear\n",
      "Saved: artifacts/model_plots_lag/\n",
      "\n",
      "Evaluating on holdout: RandomForest\n",
      "Saved: artifacts/model_plots_lag/\n",
      "\n",
      "Evaluating on holdout: XGBoost\n",
      "Saved: artifacts/model_plots_lag/\n",
      "\n",
      "Evaluating on holdout: LightGBM\n",
      "Saved: artifacts/model_plots_lag/\n",
      "\n",
      "Saved holdout performance to artifacts/model_results_lag/holdout_performance.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "RMSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAPE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "R2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b2ed019b-e959-4e40-9eb9-4fc555aa667c",
       "rows": [
        [
         "3",
         "1.5462",
         "2.3907",
         "0.9238",
         "1.8708",
         "0.9916",
         "LightGBM"
        ],
        [
         "2",
         "2.792",
         "7.7952",
         "1.8287",
         "3.6379",
         "0.9727",
         "XGBoost"
        ],
        [
         "1",
         "3.0813",
         "9.4942",
         "1.9003",
         "3.6627",
         "0.9668",
         "RandomForest"
        ],
        [
         "0",
         "3.6116",
         "13.0433",
         "2.2207",
         "4.4016",
         "0.9543",
         "Linear"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5462</td>\n",
       "      <td>2.3907</td>\n",
       "      <td>0.9238</td>\n",
       "      <td>1.8708</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>LightGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.7920</td>\n",
       "      <td>7.7952</td>\n",
       "      <td>1.8287</td>\n",
       "      <td>3.6379</td>\n",
       "      <td>0.9727</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0813</td>\n",
       "      <td>9.4942</td>\n",
       "      <td>1.9003</td>\n",
       "      <td>3.6627</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.6116</td>\n",
       "      <td>13.0433</td>\n",
       "      <td>2.2207</td>\n",
       "      <td>4.4016</td>\n",
       "      <td>0.9543</td>\n",
       "      <td>Linear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     RMSE      MSE     MAE    MAPE      R2         Model\n",
       "3  1.5462   2.3907  0.9238  1.8708  0.9916      LightGBM\n",
       "2  2.7920   7.7952  1.8287  3.6379  0.9727       XGBoost\n",
       "1  3.0813   9.4942  1.9003  3.6627  0.9668  RandomForest\n",
       "0  3.6116  13.0433  2.2207  4.4016  0.9543        Linear"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_records = []\n",
    "for name, model in best_models.items():\n",
    "    # Some entries may be raw pipelines already fitted or sklearn Pipelines unfit - ensure fitted\n",
    "    print(\"\\nEvaluating on holdout:\", name)\n",
    "    try:\n",
    "        # If model is a pipeline but not fitted, fit on training portion\n",
    "        if hasattr(model, \"fit\") and (not hasattr(model, \"predict\") or getattr(model, \"steps\", None) is None):\n",
    "            model.fit(X_train_final, y_train_final)\n",
    "\n",
    "        # Fit if not fitted (safe)\n",
    "        try:\n",
    "            # predict directly\n",
    "            y_pred = model.predict(X_test_final)\n",
    "        except Exception:\n",
    "            # fit on training then predict\n",
    "            model.fit(X_train_final, y_train_final)\n",
    "            y_pred = model.predict(X_test_final)\n",
    "    except Exception as e:\n",
    "        print(\"Model\", name, \"failed on holdout:\", e)\n",
    "        continue\n",
    "\n",
    "    mets = regression_metrics(y_test_final, y_pred)\n",
    "    mets[\"Model\"] = name\n",
    "    eval_records.append(mets)\n",
    "\n",
    "    # Plot actual vs predicted on holdout\n",
    "    fig, ax = plt.subplots(figsize=(10,3))\n",
    "    # If Date exists in df, display the last dates for x-axis\n",
    "    if \"Date\" in df.columns:\n",
    "        test_dates = df[\"Date\"].iloc[test_idx].reset_index(drop=True)\n",
    "        ax.plot(test_dates, y_test_final, label=\"Actual\")\n",
    "        ax.plot(test_dates, y_pred, linestyle=\"--\", label=\"Predicted\")\n",
    "        ax.set_xticklabels(test_dates.dt.strftime(\"%Y-%m-%d\"), rotation=45)\n",
    "    else:\n",
    "        ax.plot(y_test_final.index, y_test_final, label=\"Actual\")\n",
    "        ax.plot(y_test_final.index, y_pred, linestyle=\"--\", label=\"Predicted\")\n",
    "    ax.set_title(f\"{name} — Actual vs Predicted (Holdout)\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    save_plot(fig, f\"actual_vs_pred_{name}\")\n",
    "\n",
    "# Save eval records to CSV\n",
    "eval_df = pd.DataFrame(eval_records).sort_values(\"RMSE\")\n",
    "eval_df.to_csv(f\"{model_results_path}holdout_performance.csv\", index=False)\n",
    "print(f\"\\nSaved holdout performance to {model_results_path}holdout_performance.csv\")\n",
    "display(eval_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0611c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CV summary to artifacts/model_results_lag/cv_summary.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "RMSE_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MSE_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAE_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAPE_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "R2_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RMSE_std",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "cd761c59-faf0-441a-bd5e-e9c0c7770e7f",
       "rows": [
        [
         "0",
         "Linear",
         "10.8298",
         "185.2598",
         "8.9272",
         "16.2834",
         "-0.7448",
         null
        ],
        [
         "1",
         "RandomForest",
         "7.137",
         "60.6909",
         "4.7987",
         "8.5182",
         "0.7635",
         "0.0"
        ],
        [
         "2",
         "XGBoost",
         "7.6832",
         "74.7722",
         "5.2499",
         "8.816",
         "0.7923",
         "0.0"
        ],
        [
         "3",
         "LightGBM",
         "8.346",
         "87.6943",
         "5.5336",
         "9.5036",
         "0.723",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE_mean</th>\n",
       "      <th>MSE_mean</th>\n",
       "      <th>MAE_mean</th>\n",
       "      <th>MAPE_mean</th>\n",
       "      <th>R2_mean</th>\n",
       "      <th>RMSE_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear</td>\n",
       "      <td>10.8298</td>\n",
       "      <td>185.2598</td>\n",
       "      <td>8.9272</td>\n",
       "      <td>16.2834</td>\n",
       "      <td>-0.7448</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>7.1370</td>\n",
       "      <td>60.6909</td>\n",
       "      <td>4.7987</td>\n",
       "      <td>8.5182</td>\n",
       "      <td>0.7635</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>7.6832</td>\n",
       "      <td>74.7722</td>\n",
       "      <td>5.2499</td>\n",
       "      <td>8.8160</td>\n",
       "      <td>0.7923</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>8.3460</td>\n",
       "      <td>87.6943</td>\n",
       "      <td>5.5336</td>\n",
       "      <td>9.5036</td>\n",
       "      <td>0.7230</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  RMSE_mean  MSE_mean  MAE_mean  MAPE_mean  R2_mean  RMSE_std\n",
       "0        Linear    10.8298  185.2598    8.9272    16.2834  -0.7448       NaN\n",
       "1  RandomForest     7.1370   60.6909    4.7987     8.5182   0.7635       0.0\n",
       "2       XGBoost     7.6832   74.7722    5.2499     8.8160   0.7923       0.0\n",
       "3      LightGBM     8.3460   87.6943    5.5336     9.5036   0.7230       0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: artifacts/model_plots_lag/\n",
      "Saved: artifacts/model_plots_lag/\n"
     ]
    }
   ],
   "source": [
    "cv_df = pd.DataFrame(cv_summary)\n",
    "cv_df.to_csv(f\"{model_results_path}cv_summary.csv\", index=False)\n",
    "print(f\"Saved CV summary to {model_results_path}cv_summary.csv\")\n",
    "display(cv_df.round(4))\n",
    "\n",
    "# Plot CV RMSE comparison\n",
    "if \"RMSE_mean\" in cv_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    sns.barplot(x=\"Model\", y=\"RMSE_mean\", data=cv_df, dodge=False, ax=ax)\n",
    "    ax.set_title(\"CV: Mean RMSE by Model\")\n",
    "    save_plot(fig, \"cv_rmse_by_model\")\n",
    "\n",
    "# Plot holdout RMSE\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "sns.barplot(x=\"Model\", y=\"RMSE\", data=eval_df.rename(columns={\"RMSE\":\"RMSE\"}), dodge=False, ax=ax)\n",
    "ax.set_title(\"Holdout: RMSE by Model\")\n",
    "save_plot(fig, \"holdout_rmse_by_model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d82fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to extract RF feature importances: Cannot save file into a non-existent directory: 'artifacts\\model_results'\n"
     ]
    }
   ],
   "source": [
    "# --------- 10. Feature importance for tree models (Permutation or built-in) ---------\n",
    "# If RandomForest present, extract feature importances (via fitted pipeline)\n",
    "if \"RandomForest\" in best_models:\n",
    "    rf = best_models[\"RandomForest\"]\n",
    "    try:\n",
    "        # Get feature names after preprocessing\n",
    "        pre = rf.named_steps[\"preproc\"]\n",
    "        # numeric names and onehot names\n",
    "        num_names = numeric_cols\n",
    "        # get onehot feature names if any categories\n",
    "        try:\n",
    "            ohe = pre.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "            ohe_names = list(ohe.get_feature_names_out(cat_cols))\n",
    "        except Exception:\n",
    "            ohe_names = []\n",
    "        feat_names = num_names + ohe_names\n",
    "\n",
    "        # Extract underlying RandomForest\n",
    "        rf_est = rf.named_steps[\"est\"]\n",
    "        importances = getattr(rf_est, \"feature_importances_\", None)\n",
    "        if importances is not None:\n",
    "            imp_df = pd.DataFrame({\"feature\": feat_names, \"importance\": importances})\n",
    "            imp_df = imp_df.sort_values(\"importance\", ascending=False).head(30)\n",
    "            imp_df.to_csv(\"artifacts/model_results/rf_feature_importances.csv\", index=False)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(8,6))\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=imp_df, ax=ax)\n",
    "            ax.set_title(\"RandomForest: Top feature importances\")\n",
    "            save_plot(fig, \"rf_top_feature_importances\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to extract RF feature importances:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae8fe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running simple ARIMA baseline on target (univariate) ...\n",
      "Saved: artifacts/model_plots_lag/\n",
      "ARIMA failed: [Errno 2] No such file or directory: 'artifacts/models/arima_baseline.joblib'\n"
     ]
    }
   ],
   "source": [
    "if \"ARIMA\" in RUN_MODELS and ARIMA is not None:\n",
    "    print(\"\\nRunning simple ARIMA baseline on target (univariate) ...\")\n",
    "    try:\n",
    "        series = df.set_index(\"Date\")[TARGET].asfreq(\"D\").interpolate()\n",
    "        arima_order = (1,1,1)   # simple baseline\n",
    "        arima_model = ARIMA(series.iloc[:-n_holdout], order=arima_order).fit()\n",
    "        arima_fore = arima_model.forecast(steps=n_holdout)\n",
    "        mets = regression_metrics(series.iloc[-n_holdout:].values, arima_fore)\n",
    "        mets[\"Model\"] = \"ARIMA\"\n",
    "        # append to eval records and save plot\n",
    "        eval_df = pd.concat([eval_df, pd.DataFrame([mets])], ignore_index=True)\n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(10,3))\n",
    "        ax.plot(series.index[-n_holdout:], series.iloc[-n_holdout:], label=\"Actual\")\n",
    "        ax.plot(series.index[-n_holdout:], arima_fore, linestyle=\"--\", label=\"ARIMA_Forecast\")\n",
    "        ax.set_title(\"ARIMA: Actual vs Forecast (holdout)\")\n",
    "        ax.legend()\n",
    "        save_plot(fig, \"arima_holdout\")\n",
    "        # Save ARIMA model\n",
    "        joblib.dump(arima_model, \"artifacts/models/arima_baseline.joblib\")\n",
    "        print(\"Saved ARIMA baseline model.\")\n",
    "    except Exception as e:\n",
    "        print(\"ARIMA failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0a848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved manifest -> artifacts/model_results_lag/manifest.json\n",
      "\n",
      "Notebook 2 complete. Artifacts saved under artifacts/model_plots, artifacts/models, artifacts/model_results.\n"
     ]
    }
   ],
   "source": [
    "manifest = {\n",
    "    \"timestamp\": datetime.utcnow().isoformat(),\n",
    "    \"input_csv\": INPUT_CSV,\n",
    "    \"target\": TARGET,\n",
    "    \"features_used\": FEATURES_TO_USE,\n",
    "    \"numeric_cols\": numeric_cols,\n",
    "    \"cat_cols\": cat_cols,\n",
    "    \"models_trained\": list(best_models.keys()),\n",
    "    \"holdout_rows\": int(n_holdout)\n",
    "}\n",
    "with open(f\"{model_results_path}manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(f\"Saved manifest -> {model_results_path}manifest.json\")\n",
    "\n",
    "print(\"\\nNotebook 2 complete. Artifacts saved under artifacts/model_plots, artifacts/models, artifacts/model_results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77bdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
